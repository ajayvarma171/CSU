# -*- coding: utf-8 -*-
"""market-trends.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RFDCI8N8CY9c5E1MDUV5rRaDmQMT3dou

#Analysing market trends and consumer behaviour for new product launch
##Final project - Group 6

##Data loading and prepration
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.cluster import KMeans
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score
from statsmodels.formula.api import ols
from google.colab import drive
drive.mount('/content/drive')

# Load the dataset
sales_data = pd.read_csv('/content/drive/MyDrive/Final assignment/supermarket_sales.csv')

# Explore the structure of the dataset
print(sales_data.info())

"""#Data preprocessing"""

# Data exploration (graphs, correlations, data preprocessing)
print(sales_data.describe())

# Missing data analysis
print(sales_data.isna().sum())

"""#Exploratory Data Analysis and visualization"""

# Product line frequency analysis
product_line_counts = sales_data['Product line'].value_counts()
print(product_line_counts)

# Sales distribution
sales_distribution = sales_data.groupby('Payment')['Total'].agg(['count', 'sum'])
print(sales_distribution)

# Payment type distribution
payment_type_counts = sales_data['Payment'].value_counts()
print(payment_type_counts)

# Sales trend analysis
sales_data['Sale_Date'] = pd.to_datetime(sales_data['Date']).dt.date
sales_trend = sales_data.groupby('Sale_Date')['Total'].sum().reset_index()
plt.plot(sales_trend['Sale_Date'], sales_trend['Total'])
plt.title('Sales Trend Over Time')
plt.xlabel('Date')
plt.ylabel('Total Sales')
plt.show()

# Boxplot of product lines vs. total sales
sns.boxplot(x='Product line', y='Total', data=sales_data)
plt.title('Product Line vs. Total Sales')
plt.xlabel('Product Line')
plt.ylabel('Total Sales')
plt.show()

sales_data['Payment'] = sales_data['Payment'].astype('category')

# Group data by Payment type and calculate total sales
payment_type_sales = sales_data.groupby('Payment', observed=False, dropna=False)['Total'].sum().reset_index()

# Plot the bar chart
plt.bar(payment_type_sales['Payment'], payment_type_sales['Total'])
plt.title('Payment vs. Total Sales')
plt.xlabel('Payment Type')
plt.ylabel('Total Sales')
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt
import imageio

# Calculate the correlation matrix for the numeric columns
correlation_matrix = sales_data.select_dtypes(include=[np.number]).corr()

# Draw the correlation matrix using seaborn's heatmap function
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)

# Save the correlation matrix as a PNG image

# Sales by gender and city
sns.countplot(x='Gender', hue='City', data=sales_data)
plt.title('Customer Gender by City')
plt.xlabel('Gender')
plt.ylabel('Count')
plt.show()

# Scatterplot of product preferences by unit price and city
sns.scatterplot(x='Unit price', y='Product line', hue='City', data=sales_data)
plt.title('Product Preferences by Unit Price and City')
plt.xlabel('Unit Price')
plt.ylabel('Product Line')
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# Scatterplot of tax vs. gross income
sns.scatterplot(x='Tax 5%', y='gross income', data=sales_data)
plt.title('Relationship between Tax and Gross Income')
plt.xlabel('Tax 5%')
plt.ylabel('Gross Income')
plt.show()

"""#Model Building and Evaluation"""

from sklearn.model_selection import train_test_split

sales_data_num = sales_data.select_dtypes(include=[np.number])

X = sales_data_num.drop('Total', axis=1)
y = sales_data_num['Total']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import mean_squared_error, r2_score

# Assuming sales_data is a pandas DataFrame with numerical columns only
numerical_cols = ['gross income',"Tax 5%"] # replace with actual numerical column names
X = sales_data[numerical_cols]
y = sales_data['gross income'] # replace with actual target column name

# Perform one-hot encoding on categorical columns
categorical_cols = ['City'] # replace with actual categorical column names
encoder = OneHotEncoder(sparse=False)
X_encoded = pd.DataFrame(encoder.fit_transform(sales_data[categorical_cols]), columns=encoder.categories_[0])

# Combine numerical and encoded categorical columns
X = pd.concat([X, X_encoded], axis=1)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Linear Regression
lin_reg = LinearRegression()
lin_reg.fit(X_train, y_train)
lin_reg_pred = lin_reg.predict(X_test)
print("Linear Regression MSE: ", mean_squared_error(y_test, lin_reg_pred))
print("Linear Regression R^2: ", r2_score(y_test, lin_reg_pred))

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import mean_squared_error, r2_score

# Assuming sales_data is a pandas DataFrame with numerical columns only
numerical_cols = ['gross income',"Tax 5%"] # replace with actual numerical column names
X = sales_data[numerical_cols]
y = sales_data['gross income'] # replace with actual target column name

# Perform one-hot encoding on categorical columns
categorical_cols = ['City'] # replace with actual categorical column names
encoder = OneHotEncoder(sparse=False)
X_encoded = pd.DataFrame(encoder.fit_transform(sales_data[categorical_cols]), columns=encoder.categories_[0])

# Combine numerical and encoded categorical columns
X = pd.concat([X, X_encoded], axis=1)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Linear Regression
lin_reg = LinearRegression()
lin_reg.fit(X_train, y_train)
lin_reg_pred = lin_reg.predict(X_test)
print("Linear Regression MSE: ", mean_squared_error(y_test, lin_reg_pred))
print("Linear Regression R^2: ", r2_score(y_test, lin_reg_pred))

# Random Forest Regression
rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)
rf_reg.fit(X_train, y_train)
rf_reg_pred = rf_reg.predict(X_test)
print("Random Forest Regression MSE: ", mean_squared_error(y_test, rf_reg_pred))
print("Random Forest Regression R^2: ", r2_score(y_test, rf_reg_pred))

# K-NN Regression
knn_reg = KNeighborsRegressor(n_neighbors=3)
knn_reg.fit(X_train, y_train)
knn_reg_pred = knn_reg.predict(X_test)
print("K-NN Regression MSE: ", mean_squared_error(y_test, knn_reg_pred))
print("K-NN Regression R^2: ", r2_score(y_test, knn_reg_pred))